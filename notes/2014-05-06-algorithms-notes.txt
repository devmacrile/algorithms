Design and analysis of algorithms
Week 1 notes (part 2)

//Divide and conquer paradigm
	1) Divide into smaller sub problems
	2) Conquer via recursive calls
	
//O(nlogn) algorithm for counting inversions
	The Problem
		Input: array A containing the numbers 1,2,3,4,....,n in some arbitrary order
		Output: number of inversions: number of pairs (i,j) of array indices with i<j and A[i]>A[j]
		Example application: numerical similarity measure between two ranked lists
		
		What is the largest possible number of inversions a 6 element array may have?
			15
		In general, an n element array can have at most n choose 2 inversions
		
	High level approach
		Brute force: O(n^2) time. Can we do better?  YES.
		Key idea #1: Divide and conquer
			Call an inversion (i,j) [with i<j]:
				left if i,j <= n/2
				right if i,j > n/2
				split if i<=n/2<j
				
		High level algorithm
			Count(array A, length n)
				if n=1 return 0
				else
					x = Count(1st half of A, n/2)
					y = Count(2nd half of A, n/2)
					z = CountSplitInv(A, n)
					return x+y+z
			Goal: implement CountSplitInv in linear O(n) time => Count will run in O(nlogn) time [just like merge sort]
		
		Key idea #2: Piggybacking on merge sort
			Have recursive calls both count inversions and sort
			So x,y,z in high level algorithm sort-and-count
			
			
		Claim: the split inversions involving an element y of the 2nd array C are precisely the numbers left in the first array B when y is copied to the output of D.
		Proof: let x be an element of the first array B.
			(1) if x is copied to output D before y, then x<y => no inversion involving x & y
			(2) if y copied to output D before x, then y<x, x & y are a split inversion
			QED
		
		merge_and_CountSplitInv
			-while merging the two sorted subarrays, keep running total of number of split inversions
			-when element of 2nd array C gets copied to output D, increment total by number of elements
			Run time of subroutine: O(n) + O(n)  (first for the merge, second for the increment)
				=O(n)
				

//Matrix multiplication with divide and conquer paradigm
Asymptotic running time of iterative algorithm for matrix multiplication?
	O(n^3) (assuming can access elements in constant time)
	
Apply Divide and Conquer
Divide matrices into quadrants (A-H)
Write X = (A, B | C, D) and Y = (E, F | G, H)
Where A through H are all n/2 x n/2 matrices
Then X*Y = (AE + BG, AF + BH | CE + DG, CF + DH)

- Recursive algorithm #1
Step 1) recursively compute the 8 necessary products
Step 2) do the necessar additions (O(n^2) time)
Fact: run time is O(n^3) [follows from master method]

- Strassen's Algorithm
Step 1) recursively compute only 7 (cleverly chosen) products
Step 2) do the necessary (clever) additions + subtractions (still O(n^2) time)
Fact: better than cubic time! [see master method]

The seven products:
	P1 = A(F-H)
	P2 = (A+B)H
	P3 = (C+D)E
	P4 = D(G-E)
	P5 = (A+D)(E+H)
	P6 = (B-D)(G+H)
	P7 = (A-C)(E+F)
Claim: X*Y = (AE + BG, AF + BH | CE + DG, CF + DH)
		= (P5+P4-P2+P6, P1+P2 | P3+P4, P1+P5-P3-P7)
Proof: P5+P4-P2+P6 = AE+AH+DE+DH+DG-DE-AH-BH+BG+BH-DG-DH
	 	= AE + BG  
		QED
How on earth did Strassen come up with this (i.e. the 7 products)?
	Remains an open question!




	
		
	